{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ji8KtYOVGs8_"
   },
   "source": [
    "<p style=\"align: center;\"><img src=\"https://static.tildacdn.com/tild6636-3531-4239-b465-376364646465/Deep_Learning_School.png\" width=\"400\"></p>\n",
    "\n",
    "# Глубокое обучение. Часть 2\n",
    "# Домашнее задание по теме \"Механизм внимания\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UAr-M8_1GJ6W"
   },
   "source": [
    "Это домашнее задание проходит в формате peer-review. Это означает, что его будут проверять ваши однокурсники. Поэтому пишите разборчивый код, добавляйте комментарии и пишите выводы после проделанной работы.\n",
    "\n",
    "В этом задании вы будете решать задачу классификации математических задач по темам (многоклассовая классификация) с помощью Transformer.\n",
    "\n",
    "В качестве датасета возьмем датасет математических задач по разным темам. Нам необходим следующий файл:\n",
    "\n",
    "[Файл с классами](https://docs.google.com/spreadsheets/d/13YIbphbWc62sfa-bCh8MLQWKizaXbQK9/edit?usp=drive_link&ouid=104379615679964018037&rtpof=true&sd=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fybMcmV0YRA"
   },
   "source": [
    "**Hint:** не перезаписывайте модели, которые вы получите на каждом из этапов этого дз. Они ещё понадобятся."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(palette='summer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import matplotlib.ticker as ticker\n",
    "# from transformers import get_scheduler\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# import random\n",
    "# import math\n",
    "# import time\n",
    "# import string\n",
    "# import pymorphy3\n",
    "# import re\n",
    "\n",
    "# # datasets from huggingface\n",
    "# from datasets import load_dataset\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# from nltk.corpus import stopwords\n",
    "# import nltk\n",
    "\n",
    "# from tqdm.notebook import tqdm\n",
    "# from torch.utils.tensorboard import SummaryWriter\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# import os\n",
    "# from datetime import datetime\n",
    "# from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert/distilbert-base-uncased' \n",
    "MODEL_NAME = 'cointegrated/rubert-tiny2'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1 (2 балла)\n",
    "\n",
    "Напишите кастомный класс для модели трансформера для задачи классификации, использующей в качествке backbone какую-то из моделей huggingface.\n",
    "\n",
    "Т.е. конструктор класса должен принимать на вход название модели и подгружать её из huggingface, а затем использовать в качестве backbone (достаточно возможности использовать в качестве backbone те модели, которые упомянуты в последующих пунктах)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_bert_tokenizer_and_embedder(bert_model_name, device=device):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
    "    # bert_embedder = AutoModelForSeq2SeqLM.from_pretrained(bert_model_name)\n",
    "    bert_embedder = BertModel.from_pretrained(bert_model_name)\n",
    "    bert_embedder.pooler = nn.Identity()\n",
    "    del bert_embedder.encoder.layer[:]\n",
    "\n",
    "    return tokenizer, bert_embedder.to(device)\n",
    "\n",
    "tokenizer, embedder = prepare_bert_tokenizer_and_embedder(bert_model_name=MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(83828, 312, padding_idx=0)\n",
       "    (position_embeddings): Embedding(2048, 312)\n",
       "    (token_type_embeddings): Embedding(2, 312)\n",
       "    (LayerNorm): LayerNorm((312,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList()\n",
       "  )\n",
       "  (pooler): Identity()\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download pretrained models\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=4).to(device)\n",
    "for w in model.distilbert.parameters():\n",
    "    w._trainable= False\n",
    "for w in model.classifier.parameters():\n",
    "    w._trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eX4VGWquyiMx"
   },
   "outputs": [],
   "source": [
    "### This is just an interface example. You may change it if you want.\n",
    "\n",
    "class TransformerClassificationModel(nn.Module):\n",
    "    def __init__(base_transformer_model: Union[str, nn.Module]):\n",
    "        self.backbone = #...\n",
    "        # YOUR CODE: create additional layers for classfication\n",
    "\n",
    "    def forward(inputs, ...):\n",
    "        # YOUR CODE: propagate inputs through the model. Return dict with logits\n",
    "\n",
    "        outputs = {<YOUR CODE>}\n",
    "        return # YOUR CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vd3kxX6hy0d4"
   },
   "source": [
    "### Задание 2 (1 балл)\n",
    "\n",
    "Напишите функцию заморозки backbone у модели (если необходимо, возвращайте из функции модель)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U8IuDosbzKe8"
   },
   "outputs": [],
   "source": [
    "def freeze_backbone_function(model: TransformerClassificationModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kybkw6MSzd-K"
   },
   "source": [
    "### Задание 3 (2 балла)\n",
    "\n",
    "Напишите функцию, которая будет использована для тренировки (дообучения) трансформера (TransformerClassificationModel). Функция должна поддерживать обучение с замороженным и размороженным backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EDhrD0BHzxi4"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def train_transformer(transformer_model, freeze_backbone=True)\n",
    "    model = copy.copy(transformer_model)\n",
    "    ### YOUR CODE IS HERE\n",
    "\n",
    "    return finetuned_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUqhI4mV_RTI"
   },
   "source": [
    "### Задание 4 (1 балл)\n",
    "\n",
    "Проверьте вашу функцию из предыдущего пункта, дообучив двумя способами\n",
    "*cointegrated/rubert-tiny2* из huggingface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nuxOCBQHAKZC"
   },
   "outputs": [],
   "source": [
    "rubert_tiny_transformer_model = #...\n",
    "rubert_tiny_finetuned_with_freezed_backbone = train_transformer(rubert_tiny_transformer_model, freeze_backbone=True)\n",
    "\n",
    "rubert_tiny_transformer_model = #...\n",
    "rubert_tiny_full_finetuned = train_transformer(rubert_tiny_transformer_model, freeze_backbone=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRi7tkoOAjon"
   },
   "source": [
    "### Задание 5 (1 балл)\n",
    "\n",
    "Обучите *tbs17/MathBert* (с замороженным backbone и без заморозки), проанализируйте результаты. Сравните скоры с первым заданием. Получилось лучше или нет? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XKtd3YgNA14E"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE IS HERE (probably, similar on the previous step)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuU6Di26017B"
   },
   "source": [
    "### Задание 6 (1 балл)\n",
    "\n",
    "Напишите функцию для отрисовки карт внимания первого слоя для моделей из задания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "guzGxfcV1Cba"
   },
   "outputs": [],
   "source": [
    "def draw_first_layer_attention_maps(attention_head_ids: List, text: str, model: TransformerClassificationModel):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu0adKw4BLtF"
   },
   "source": [
    "### Задание 7 (1 балл)\n",
    "\n",
    "Проведите инференс для всех моделей **ДО ДООБУЧЕНИЯ** на 2-3 текстах из датасета. Посмотрите на головы Attention первого слоя в каждой модели на выбранных текстах (отрисуйте их отдельно).\n",
    "\n",
    "Попробуйте их проинтерпретировать. Какие связи улавливают карты внимания? (если в модели много голов Attention, то проинтерпретируйте наиболее интересные)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2gEF3vkB6eR"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE IS HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBNVrOpCCLqk"
   },
   "source": [
    "### Задание 8 (1 балл)\n",
    "\n",
    "Сделайте то же самое для дообученных моделей. Изменились ли карты внимания и связи, которые они улавливают? Почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5229WBICWEr"
   },
   "outputs": [],
   "source": [
    "### YOUR CODE IS HERE"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1q77WixYXvCFQtvRD-RbCQe7zC06hn7d7",
     "timestamp": 1708365361858
    },
    {
     "file_id": "12BxEICwjMhOcIQID1zn06K6kFG92GZcf",
     "timestamp": 1708035269638
    },
    {
     "file_id": "1pYR9hzeFxq1T5kZThLvgA7bkmcNQtbrq",
     "timestamp": 1707661673565
    }
   ]
  },
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
